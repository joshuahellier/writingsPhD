\chapter{Transition Rate Matrix Analysis} 
\label{sec:transRateChapter}
Now that we have MFT predictions about the relationship between density difference and
current in the SPM, it would be good to try to investigate their validity. In Chapter~\ref{sec:numerics} we will use Monte-Carlo methods to do this in $1$d and $2$d, but in this chapter we will restrict our attention to $1$d.

\section{The Transition Rate Operator for the SPM} \label{sec:TRMGeneralResults}
The SPM is an autonomous continuous-time Markov Process, which describes continual transitions between states
with transition rates depending only upon the current state. As such, if we call the total space of
states $\Xi$ then the probability distribution $P: \Xi \times \mathbb{R} \rightarrow  \mathbb{R}$ should obey a \textbf{master equation}
\begin{equation} \label{eq:masterEq}
 \partDeriv{P(\xi, t)}{t} = \mathcal{A} P(\xi, t),
\end{equation}
where $\mathcal{A}:\Xi \rightarrow \Xi$ is the \textbf{transition rate operator}
or TRO. Note that I am going to be using column vectors for probabilities rather than
row vectors (as many in the probability community do) because it is what I am used to.
Parametrising $\mathcal{A}$ via 
\begin{equation}
 (\mathcal{A}f)(u) = \int_\Xi \! \! \mathrm{d}  \xi \ \sigma (u, \xi) f(\xi)
\end{equation}
puts it in a more familiar, transport equation-style notation:
\begin{equation}
 \partDeriv{P(\xi, t)}{t} = \int_\Xi \! \! \mathrm{d}  u \ \sigma (\xi, u) P(u, t)
\end{equation}

We demand that $\sigma$ satisfies
\begin{equation}
 \forall \xi \in \Xi, \ \sigma (\xi , \xi) \le 0 
\end{equation}
and
\begin{equation}
 \forall \xi_1 , \xi_2 \in \Xi : \xi_1 \ne \xi_2 , \ \sigma (\xi_1 , \xi_2) \ge 0 ,
\end{equation}
as well as the constraint
\begin{equation}
 \forall u \in \Xi , \ \int_\Xi \! \! \mathrm{d}  \xi \ \sigma (\xi, u) = 0.
\end{equation}
The last constraint implies that
\begin{equation}
 \int_\Xi \! \! \mathrm{d}  \xi \partDeriv{P(\xi, t)}{t} = \int_\Xi \! \! \mathrm{d}  u \ P(u, t) \left[ \int_\Xi \! \! \mathrm{d}  \xi \ \sigma (\xi, u) \right] = 0
\end{equation}
regardless of the structure of $P$, which is our probability conservation equation.

The formal (forward-time) solution to Eq.~\ref{eq:masterEq} is given by
\begin{equation}
 P(\xi, t) = e^{(t-t_0)\mathcal{A}}P_0,
\end{equation}
where $t_0$ is some initial time, $P_0$ is the starting distribution, and the operator
exponential is defined by its Taylor expansion, which should converge fine for bounded
$\mathcal{A}$, satisfied for the finite-system SPM. As 
$e^{(t-t_0)\mathcal{A}}$ and $\mathcal{A}$ share eigenvectors, we see that the
eigenstructure of $\mathcal{A}$ is something well worth investigating, as it should
give us information about the time-evolution of the system. An important thing to point
out is that $\mathcal{A}$ does not in general have orthogonal eigenvectors because it is
not in general symmetric,
and so we cannot normally diagonalise it using orthogonal transformations. This means that modes do not ``decouple'' in the way that states do in the 
Schr\"{o}dinger equation, and we instead have to deal with an \textbf{adjoint system}.

Luckily, when it comes to the analysis of steady states, there are a few results that
can help us. If we consider the operator $\mathcal{G}_T = e^{T\mathcal{A}}$ (in other words, the 
\textbf{propagator} for a period of time $T$), it is pretty easy to see that this is
a standard Markov Operator, as we are essentially reversing the limiting process
we would perform in order to define a continuous time Markov process as a limit of
a discrete time one. If we assume that $\Xi$ is finite and $\mathcal{G}_T$ is irreducible then as
a Markov operator it must have a  a unique eigenvector with corresponding eigenvalue $1$.
All other eigenvalues must have modulus between $0$ and $1$. Therefore, $\mathcal{A}$
must share that same unique eigenvector with associated eigenvalue $0$, and its other
eigenvectors must have negative real part. In other words, the system must have a single
steady state probability distribution, which it always relaxes towards exponentially
quickly, with a rate determined by the nonzero eigenvalue of $\mathcal{A}$ with real
part closest to $0$.

Of course, for such finite-state systems (such as the SPM on a finite domain) the integrals become sums, and $\sigma$ a matrix, $Q$. In such systems, we can arrange to have some labelling scheme which uniquely relates system states to natural numbers, and therefore
relates states to basis vectors in a vector space. In the SPM, a site is either full or empty, which means that there is a natural mapping between states and natural numbers based upon binary representation;
a string of $1$s and $0$s can be associated with a natural number as well as a configuration of particles and vacancies.

\subsection{A Small Worked Example}

As a concrete example, let us consider the SPM on a cyclic domain of length $3$. There
are $2^3$ possible combinations, and so $8$ possible states: $000$, $001$, and so forth.
The transition rate matrix describing this system is:
\begin{equation}
 Q =
 \begin{bmatrix}
0  &   &   &           &   &   &   &   \\
   &-2 & 1 &           & 1 &   &   &   \\
   & 1 &-2 &           & 1 &   &   &   \\
   &   &   & -2\lambda &   &  \lambda &  \lambda &   \\
   & 1 & 1 &           & -2&   &   &   \\
   &   &   &  \lambda    &   &  -2\lambda & \lambda  &   \\
   &   &   &  \lambda  &    & \lambda  & -2 \lambda &   \\
   &   &   &           &   &   &   & 0  \\
\end{bmatrix},
\end{equation}
where we have omitted most of the zero entries for clarity.
An alert observer will note that $Q$ is reducible, and so by permuting the basis vectors
we can rearrange the matrix into block form:
\begin{equation}
 Q ' = 
 \begin{bmatrix}
-2  & 1 & 1 &           &   &   &   &   \\
 1 &-2 & 1 &           &  &   &   &   \\
 1 & 1 &-2 &           &  &   &   &   \\
   &   &   & -2\lambda & \lambda &  \lambda &   &    \\
   &   &   &      \lambda& -2\lambda   &  \lambda &   \\
   &   &   &  \lambda   &   -2\lambda & \lambda & &   \\
   &   &   &       &  &  & 0 &  \\
   &   &   &           &   &   &   & 0  \\
\end{bmatrix}.
\end{equation}

According to our recipe, to learn about the solutions to Eq.~\ref{eq:masterEq} 
in this case, we need to know about the eigendecomposition of $Q$. 
The block structure of $Q'$ means that there are $4$ 
distinct parts of the state space, between which there are no transitions; this 
partitioning corresponds to the fact that particle number is conserved on the ring.
Two of these
sectors correspond to the full and empty states, and their dynamics are completely
trivial, in the sense that the state space is $1$d and there are no dynamics, as the
particles/vacancies have nowhere to go!
The remaining sectors correspond to the situation where there is one particle or one 
vacancy. The matrix is symmetric, meaning that its eigenvalues are real,
and we find that both nontrivial blocks can be diagonalised to form a multiple of
\begin{equation}
 \begin{bmatrix}
  0  &  &  \\
     &-3&  \\
     &  &-3\\
 \end{bmatrix},
\end{equation}
with eigenvectors $[1, 1, 1]^{\mathrm{T}}$, $[-1, 0, 1]^{\mathrm{T}}$ and
$[-1, 1, 0]^{\mathrm{T}}$ respectively, the latter two forming a degenerate eigenspace.

In this particular example then, we find that there $4$ steady states:
\begin{itemize}
 \item All slots full,
 \item All slots empty,
 \item One particle present, with equal chance to be in any particular position,
 \item The same but with a vacancy instead of a particle.
\end{itemize}
Whilst the first 2 cases are trivial (one-dimensional probability spaces), in the other
two cases we relax towards the steady state with rates $3$ and $3\lambda$ respectively.
In this example, the TRM was actually symmetric, and so all the eigenvalues were real.
It was also highly reducible, due to the strict constraints imposed by the particle
conservation law. When we have boundary conditions which permit the creation and
destruction of particles, that will change; we will consider that situation now.

\section{Forming the TRM for Systems with Dirichlet Boundary Conditions}

\subsection{Dirichlet Boundary Conditions}
In Chap.~\ref{sec:analChap} we made an MFT for the SPM in $1$-dimension, and when 
investigating steady states used Dirichlet boundary conditions when we needed them.
In particular, we had a system of length $L$ and sought solutions in which the system
density was pinned at $\rho_0$ at $x=0$ and $\rho_L$ at $x=L$. 

We would like to do a similar thing for the non-MFT SPM. An exact analogue of the situation
does not exist, as occupation number is not defined \textit{a priori} in a Markov process, but merely emerges as a result of the rate prescribed.The closest imitation to it
we can get is by allowing particles to be created and destroyed in boundary regions at
either end of a chain, and then try to set these rates so that the time-averaged occupation probability in the end sites are $\rho_0$ and $\rho_L$ respectively. Note
that this is a little more involved than simply loading and unloading particles as
one does in ASEP, as
\begin{itemize}
 \item loading and unloading really doesn't simulate the boundary condition we are looking for, and
 \item we actually need to consider a two-site boundary layer attached to each end, because the internal dynamics of the particles depend upon their immediate environment.
\end{itemize}
To simulate a boundary which is attached to a reservoir with occupation $\rho$, we allow
particles to appear in empty boundary sites with rate $B_0 \sqrt{\frac{\rho}{1-\rho}}$
and to disappear from full ones with rate $B_0 \sqrt{\frac{1-\rho}{\rho}}$,
for some positive $B_0$. If we switch
off all other dynamics and consider this in isolation, we would have a bunch of
decoupled two-state systems. Writing a TRM for this with the first basis vector being
the empty state and the second the full one, we get
\begin{equation}
 Q = 
 \begin{bmatrix}
  - B_0 \sqrt{\frac{\rho}{1-\rho}} & B_0 \sqrt{\frac{1-\rho}{\rho}} \\
  B_0 \sqrt{\frac{\rho}{1-\rho}} & -B_0 \sqrt{\frac{1-\rho}{\rho}} \\
 \end{bmatrix}.
\end{equation}
There is of course a zero eigenvalue, which corresponds to a stationary distribution
$ [1-\rho, \rho]^\mathrm{T} $, precisely as desired.

In order to simulate attaching a reservoir with particle density $\rho$, we apply
the creation and annihilation rates above to the outermost two sites in our lattice.
The outermost site only performs these operations. The inner boundary site undergoes
these creation and annihilation processes, as well as the normal dynamics of the SPM;
thus, particles will move in and out of it as normal, and when the outermost site's
occupation is required to determine the transition rate for a particle moving inward,
that information is available. One could possibly eliminate the need for the outermost
boundary site by averaging over occupations, however we have chosen not to do this for
consistency with out Monte-Carlo calculations in Chap.~\ref{sec:numerics}.

Observant readers will notice that by using these creation and annihilation rates we
are left with a free variable, $B_0$. This controls the ratio of the creation and
annihilation rates to the internal dynamical rates $1$ and $\lambda$. In general
when we're trying to simulate an adjacent reservoir we want the boundary motion to be
``fast'' compared to the internal dynamics, and so $B_0$ may be regarded as a
regularisation parameter; any choice of $B_0$ should be good so long as
the creation and annihilation rates sufficiently dominate both $1$ and $\lambda$.
In practise, in our larger-scale calculations we  used
\begin{equation}
 B_0 = b(1+\lambda),
\end{equation}
with $b$ set to, $100$ or $1000$.

\subsection{Formation of the TRM in Sparse Format}

The smallest SPM system with boundary conditions implemented using the above method is
of size $L=4$, and so has a state space of dimension $2^4=16$. This already a bit too large for us to seriously consider solving analytically. Instead, let us try to develop
a method for the numerical analysis of TRMs of arbitrary size.

The important thing to note about transition rate matrices for local lattice models
such as the SPM is that whilst the TRM itself grows very aggressively with system size, the TRM is generally extremely sparse.
The state space dimension grows as $2^L$, and the TRM dimension therefore grows as
$2^L \times 2^L$. However, a state containing $N$ particles only has transitions to
\begin{itemize}
 \item states which differ from the current state by one particle move, of which there
 are $\mathcal{O}(N)$, and
 \item states which differ from the current state by a single particle creation or
 annihilation, of which there are $\mathcal{O}(4)$.
\end{itemize}
Thus, as $N \le L$ the number of nonzero entries in the matrix is 
$\mathcal{O}(2^{L}L)$,
which is not a particularly tight bound. Therefore, the overall density of the TRM
is $\mathcal{O}(2^{-L}L)$. Given that there already exist a great many efficient
numerical routines for sparse linear algebra operations, there exists the possibility
that we could use this to solve the SPM on a finite domain for small systems
``exactly'' (or at least, up to some nominated numerical tolerance).

To make use of this, we need to assemble the TRM in a suitable sparse format.
We have written a Python code which does this. The script itself is stored <decide how 
precisely>, but the gist of the algorithm is to simply run through all possible states,
document all the transitions they can perform, and store the resulting entries in
a sparse matrix element by element. During construction, the matrix should be stored in
coordinate list format, i.e. a list of elements of the form $(\mathrm{row}, 
\mathrm{column}, \mathrm{value})$, as this is trivial to update as we only inspect
each matrix entry once. We can then convert the matrix to a compressed format such
as CSC (Compressed Sparse Column) or CSR (Compressed Sparse Row). We used CSC, but in
hindsight CSR would probably have been a better choice as it tends to make
matrix-vector multiplication a little more efficient. Once we have the TRM in this
format, it is ready for sparse linear algebra operations. Note that these whilst
these operations generally happen ``in place'', this part of the process is the
memory-intensive bit, as of course the memory usage scales with the number of nonzero
matrix elements, which is $\mathcal{O}(2^{L}L)$.
In terms of actual numbers, we found
that 4G of memory was more than adequate to solve a system of $16$ sites in total;
as the memory required to represent the TRM is the main limiting factor in this kind of
calculation, we would recommend computing on machines with large working memories,
e.g. DiRAC.

\section{The Eigenspectrum of the TRM}

\subsection{The Computation of the TRM Eigenspectrum}
Once we have the TRM $Q$ in CSC or CSR format, we can then use sparse linear algebra. 
In our code we called the Python routine \texttt{scipy.sparse.linalg.eigs} upon it,
which itself is a wrapper for C codes which find eigenpairs according to desired
criteria; precisely which algorithm to used is determined during runtime, and it may
try different methods if it doesn't initially succeed.

In our computations, we typically performed two types of calculation. In the first we merely sought to
find the steady state, so which we requested only the eigenvector $x_0$
associated to the
eigenvalue $q_0$ with smallest absolute value, which should always be numerically zero.
We requested that the eigenpair be found to a relative accuracy $\epsilon$ accuracy of $1$ part in $10^{12}$,
which amounts to saying that
\begin{equation} \label{eq:linAlgRelAcc}
 \frac{\| Q x_0 - q_0 x_0 \|}{\| x_0 \|} \le 10^{-12} = \epsilon
\end{equation}
where $\| \cdot \|$ is some reasonable subordinate matrix norm (in our case, the $1$ norm). Because we requested the eigenvalue closest to $0$, \texttt{eigs} used the
shift-invert method <find article>, leading to greater accuracy in the computation of
eigenvalues near to $0$ which is exactly what we wanted. For the other type of
computation, we instead requested the $k$ eigenpairs with largest real part, which
most likely provoked the code to use a Implicitly Restarted Arnoldi Method (IRAM).
This is not as accurate for computing the steady state, but yields vastly more accurate
results when computing the other eigenpairs compared to the first method. 

\subsection{The Structure of the TRM Eigenspectrum}

Using the code listed at <place>, we can compute the eigenspectrum of an SPM system
with boundary densities connected at both ends. Such a computation requires the
following parameters to be specified:
\begin{itemize}
 \item $\rho_0 \in (0, 1)$, the density of the reservoir connected to the left end of the domain.
 \item $\rho_L \in (0, 1)$, the density of the reservoir connected to the right end of the domain.
 \item $L \in \mathbb{N}$, the system size. The way we have defined things in the code, we do not
 count the two sets of two particles representing the boundary; thus, $L=1$ actually
 refers to a system which contains $5$ lattice sites, of which $4$ are busy doing
 boundary duties.
 \item $b>0$, the variable which controls the separation of timescales between the 
 flickering motion on the boundaries and the internal motion within the bulk of the
 system. This should be set to something large, and compared to other values of it
 to ensure that it is working as a regularisation parameter
 (i.e. large changes to $b$ have minimal impact on the relevant internal
 dynamics of the system, even if they have a great impact on the boundary dynamics).
 \item $\lambda>0$, the internal anomalous movement rate in the SPM.
 \item $\epsilon$, the relative accuracy the calculation aims for in the sense of Eq.~\ref{eq:linAlgRelAcc}.
 \item $1 \le k < 2^{L+4}$, the number of eigenvalues to compute.
 \end{itemize}

 For consistency we kept $L$, $b$, $\epsilon$ and $k$ constant during runs of 
 calculations. This leaves $\rho_0$, $\rho_L$ and $\lambda$ to be varied. In terms of what to vary and how to display the data, we decided to
 allow the spectrum to depend upon only one variable, and picked $\lambda$ to be that
 variable. We generally chose $\rho_0 =0.6$ and $\rho_L =0.4$ in order to study a system in which,
 at least for $\lambda=1$, the density is middling and a current flows.
 We then computed the resulting eigenspectrum in a couple different ways.
 First, we performed a relatively small calculation, in terms of the number of eigenvalues demanded and the number of $\lambda$ used. We altered the values of $L$
 and $b$ between runs, so that we can see what impact they have on the eigenspectrum. We then performed a very
 large calculation, in order to get a good look at the eigenstructure as a whole.
 In Chap.~\ref{sec:analChap} our MFT suggested that a transition might occur around
 $\lambda=\frac{1}{4}$, so we should look out for odd behaviour in that regime.

 \subsubsection{Small Calculations}
 First, we performed a calculation with $L=5$, $b=1000$ in which we computed the
 negative real part of the $32$ eigenvalues with real part closest to zero for a wide range of $\lambda$ with fixed
 boundary conditions. The resulting eigenspectrum is displayed in
 Fig.~\ref{fig:eigsL5B1000}.
 \begin{figure}[!ht]
 \caption[The TRM eigenspectrum for a system with $L=5$, $b=1000$.]{\label{fig:eigsL5B1000} 
 The TRM eigenspectrum, computed for a system with $\rho_0 = 0.6$, $\rho_L = 0.4$,
 $L=5$, $b=1000$.
 }
  \begin{center}
 \includegraphics[width=0.9\textheight, angle=270]{TRM/images/singleMultEigB1000L5}
  \end{center}
\end{figure}
Of course, for such a system we expect that all eigenvalues have negative real part,
as we discussed in~\ref{sec:TRMGeneralResults}, so we have chosen to plot the
negative real part divided by $\lambda$ as a function of $\lambda$; the zero eigenvalue,
which we expect to exist and is generally found to exist in numerical terms, has been ignored, as its magnitude is simply an artefact of the numerical calculation and
is therefore meaningless except possibly as a check on the numerics. The decision 
to divide by $\lambda$ was taken because the eigenvalue closest to $0$ (which dominates
approach to equilibrium) mostly scales as $\mathcal{O}(\lambda)$, so plotting without division
uses graph space poorly.

There are a several things to note about this spectrum:
\begin{itemize}
 \item It would appear that, for all $\lambda$, there is a single eigenvalue which
 is closest to $0$ and undergoes no crossing as $\lambda$ varies. At both extremes of
 $\lambda$, it clearly scales as $\mathcal{O}(\lambda)$, but is unusually low in an intermediate regime of $\lambda \in (0.03, 10)$, reaching a minimum at $\lambda \sim 0.3$. This is
 important, as the eigenvalue with smallest negative real part is the one that controls
 the approach to equilibrium; thus, we can see that around $0.3$ the system should taken unusually long to relax from an arbitrary prepared state to equilibrium.
 \item All eigenvalues tend to scale as $\mathcal{O}(\lambda)$ as $\lambda \rightarrow \infty$.
 \item Eigenvalues other than the bottom one tend to scale as $\mathcal{O}(1)$ at small
 $\lambda$. Thus, whilst the overall approach to equilibrium occurs with rate 
 $\mathcal{O}(\lambda)$, there are still plenty of (probably more localised) processes 
 occurring over much quicker timescales, whilst the whole system is sluggish to 
 equilibrate.
 \item The eigenvalues tend to retain their order at the extremes of $\lambda$, 
 but they frequently cross, merge and separate again in the intermediate regime. This suggests that something complicated is occurring; an analytic solution to this dynamical model would need to describe these crossings in detail, as they are important as
 eigenvalue crossing implies the appearance of degenerate eigenspaces in the decay modes
 which the associated eigenvectors refer to. This makes it look rather 
 unfeasible that such an analytic solution could be constructed in the first place.
\end{itemize}
Of course, this is only the $32$ lowest eigenvalues; there could be different behaviour
in the higher ones. Before we perform a larger calculation however, let us turn our
attention to the dependence of the eigenspectrum upon $b$ and $L$, which we have studied
by repeating our calculation with different values for those parameters, as shown in
Fig~\ref{fig:eigsVarBVarL}.
 \begin{figure}[h!]
 \caption[The TRM eigenspectrum for a system with $L=\{5, 10\}, $, $b=\{100, 1000\}$.]{\label{fig:eigsVarBVarL} 
 The TRM eigenspectrum, computed for a system with $\rho_0 = 0.6$, $\rho_L = 0.4$,
 with all combinations of
 $L=\{5, 10\}$, $b=\{100, 1000\}$. Missing computations, visible via the vertical gaps in
 the data, are due to computational issues, rather than being numerically meaningful.
 }
  \begin{center}
 \includegraphics[width=1.0\textheight, angle=270]{TRM/images/repeatRepeatMultEig}
  \end{center}
\end{figure}
Again, there's a lot going on in this image, so let's break it down.
\begin{itemize}
 \item Firstly, let us note that keeping $L$ constant and switching $b$ between $100$
 and $1000$ seems to have very little impact on the eigenspectrum, as we can see by the fact that the points corresponding to the same system sizes are
 more or less lying on top of each other unless we delve into the low-$\lambda$ limit
 of the $L=5$ system. In the same limit, we see that the discrepancy is vastly reduced
 in the $L=10$ system, suggesting that it is less bad for large systems, and is in some sense a small-size effect. Regardless, it would appear to be the case that adjusting $b$
 doesn't affect these low-lying eigenvalues very much, so it seems to be playing its
 role as a regularisation parameter as intended. Thus, we're simply going to use $b=100$
 from now on, unless explicitly specified otherwise.
 \item The bottom eigenvalue, which controls relaxation to equilibrium, is generally
 several orders of magnitude lower for the $L=10$ system than for the $L=5$ one;
 this effect is particularly extreme in the limit of small $\lambda$.
 This makes sense; the larger system should take much longer to equilibrate than the
 smaller one, as so many more fluctuations need to occur to make it change its global
 state. This is something we should remember for later, as it suggests that equilibration time might scale
 pretty aggressively with system size, which is bad news for our attempts to simulate
 the system using Monte-Carlo methods (Chap.~\ref{sec:numerics}).
 \item The big, obvious difference between the spectra of different system size is
 the behaviour at small $\lambda$. For $L=5$ there is only $1$ eigenvalue which scales
 as $\mathcal{O}(\lambda)$ as $\lambda \rightarrow 0$, whereas for $L=10$, it would
 appear that they all do!
\end{itemize}
The last point begs the following question: how does the number of eigenvalues
corresponding to ``slow modes'' at small-$\lambda$ scale with the system size? This is 
important,
because it determines how many of the available decay modes would tend to persist for any 
meaningful amount of time during the relaxation towards equilibrium. We will attempt to
address this in the next subsection.

\subsubsection{The Scaling of the Number of Slow Modes at Low $\lambda$}
To find out how the number of nonzero eigenvalues in the $\mathcal{O}(\lambda)$-scaling
band (as observed in Fig.~\ref{fig:eigsVarBVarL}) scales with system size,
we can simply fix $\lambda$ to be sufficiently low (say, $\lambda=0.0001$)and repeat
computations of the low-lying eigenspectrum for different $L$. Here, due to the
computational limitations we are up against, we only compute up to $L=13$. The number
of slow modes as a function of $L$ is displayed in Tab.~\ref{tab:bandThickness}.
\begin{table} \caption[Tabulated values for the variation of the width of the slow band with
system size.]{A table of the number of slow modes occurring at low $\lambda$ for given 
$L$.}
\begin{tabular}{r || r | r | r | r | r | r | r | r | r | r | r | r} \label{tab:bandThickness}
 $L$ & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 \\
 \hline
 Bandwidth & 1 & 3 & 6 & 11 & 20 & 36 & 64 & 113 & 199 \\
\end{tabular}
\end{table}
If we plot the number of slow modes as a function of system size on a plot with a 
logarithmic $y$-axis, as we have done in Fig.~\ref{fig:bandWidthScaling},
it is rather obvious that the bandwidth scales exponentially with system size.
However, the number of slow modes seems to scale as $\mathcal{O}(2^{\sim 0.84 L})$,
whereas the number of possible states, and therefore the total number of eigenvalues of
the TRM, scales as $2^L$. Thus, one can see that although the number of slow modes grows
very aggressively with the system size, it would appear to eventually become 
outpaced by the growth in the total number of eigenvalues; in this way, we can say that
the slow band carries increasingly less of the total eigenvalue density as system size
becomes large.
 \begin{figure}[h!]
 \caption[A graph of the scaling of the number of slow modes with system size.]{\label{fig:bandWidthScaling} 
 A plot of the scaling of the number of slow modes in the eigenspectrum of the TRM of
 an SPM system of size $L$. The trendline displayed has equation
 $ \mathrm{Bandwidth} = 2^{a(L+b)}$, with $a=0.84$, $b=-3.9$.
 }
  \begin{center}
 \includegraphics[width=1.0\textwidth]{TRM/images/bandWidth}
  \end{center}
\end{figure}

\subsubsection{Large Calculation}

\subsection{Current and Density in the Steady State}

\section{Time-Dependent Properties of Small SPM Systems}

\subsection{The Relaxation Time for the SPM}

\subsection{Autocorrelation Functions for the TRM}

\section{Conclusions}
